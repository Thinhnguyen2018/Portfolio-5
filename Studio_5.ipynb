{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X7AtpJvLGXKe"
      },
      "outputs": [],
      "source": [
        "pip install tensorflow numpy mnist"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import section\n"
      ],
      "metadata": {
        "id": "L908jknVsxt2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lznXGzMMqWoQ",
        "outputId": "0b22e122-c743-4b8c-dbe8-64a5fbb1eb26"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/My Drive/Portfolio 5/Corrosion"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zlRSA1Zcqz4x",
        "outputId": "ce6931d7-f2d4-4119-8e0a-1ac8c264b44b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/My Drive/Portfolio 5/Corrosion\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import random\n",
        "import shutil\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "8tTgK30gqG9p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip Corrosion.zip -d \"/content/drive/My Drive/Portfolio 5\"\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5XrMm967s-Hc",
        "outputId": "43828343-33f5-4f21-b6f6-999712b7d074"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "unzip:  cannot find or open Corrosion.zip, Corrosion.zip.zip or Corrosion.zip.ZIP.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Data Preparation**"
      ],
      "metadata": {
        "id": "Q6kdqrgF0QAI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_dir ='/content/drive/My Drive/Portfolio 5/Corrosion/Corrosion'\n",
        "\n",
        "rust_images = [os.path.join(data_dir, 'rust', f) for f in os.listdir(os.path.join(data_dir, 'rust'))]\n",
        "no_rust_images = [os.path.join(data_dir, 'no_rust', f) for f in os.listdir(os.path.join(data_dir, 'no_rust'))]\n",
        "\n",
        "\n",
        "# Splitting images into training and test sets (10 rust, 10 no rust for test set)\n",
        "rust_train, rust_test = train_test_split(rust_images, test_size=10, random_state=42)\n",
        "no_rust_train, no_rust_test = train_test_split(no_rust_images, test_size=10, random_state=42)\n",
        "\n",
        "train_set = rust_train + no_rust_train\n",
        "test_set = rust_test + no_rust_test\n",
        "\n",
        "# Create directories to store train/test sets\n",
        "os.makedirs('train/rust', exist_ok=True)\n",
        "os.makedirs('train/no_rust', exist_ok=True)\n",
        "os.makedirs('test/rust', exist_ok=True)\n",
        "os.makedirs('test/no_rust', exist_ok=True)\n",
        "\n",
        "# Move images to appropriate directories\n",
        "for img in rust_train:\n",
        "    shutil.copy(img, 'train/rust/')\n",
        "for img in no_rust_train:\n",
        "    shutil.copy(img, 'train/no_rust/')\n",
        "for img in rust_test:\n",
        "    shutil.copy(img, 'test/rust/')\n",
        "for img in no_rust_test:\n",
        "    shutil.copy(img, 'test/no_rust/')"
      ],
      "metadata": {
        "id": "CSsatoCOs_Ej"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "train_datagen = ImageDataGenerator(rescale=1./255, horizontal_flip=True, vertical_flip=True)\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "# Create training and testing datasets\n",
        "train_data = train_datagen.flow_from_directory(\n",
        "    'train/',\n",
        "    target_size=(128, 128),\n",
        "    batch_size=32,\n",
        "    class_mode='binary'\n",
        ")\n",
        "\n",
        "test_data = test_datagen.flow_from_directory(\n",
        "    'test/',\n",
        "    target_size=(128, 128),\n",
        "    batch_size=32,\n",
        "    class_mode='binary'\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8kroPgHH2iWc",
        "outputId": "c254118b-ce72-4369-f155-361cdc2b935a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 752 images belonging to 2 classes.\n",
            "Found 20 images belonging to 2 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task 1**"
      ],
      "metadata": {
        "id": "GK-9grAA7qq4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**CNN Model**"
      ],
      "metadata": {
        "id": "ijlmcSrM7zWe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "\n",
        "def create_simple_cnn():\n",
        "    model = Sequential([\n",
        "        Conv2D(32, (3, 3), activation='relu', input_shape=(128, 128, 3)),\n",
        "        MaxPooling2D(2, 2),\n",
        "        Conv2D(64, (3, 3), activation='relu'),\n",
        "        MaxPooling2D(2, 2),\n",
        "        Conv2D(128, (3, 3), activation='relu'),\n",
        "        MaxPooling2D(2, 2),\n",
        "        Flatten(),\n",
        "        Dense(128, activation='relu'),\n",
        "        Dropout(0.5),\n",
        "        Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# Instantiate and train the model\n",
        "simple_cnn = create_simple_cnn()\n",
        "history = simple_cnn.fit(train_data, epochs=10, validation_data=test_data)\n",
        "\n",
        "# Save the model\n",
        "simple_cnn.save('simple_cnn_model.h5')\n",
        "\n",
        "# Evaluate the model\n",
        "test_loss, test_acc = simple_cnn.evaluate(test_data)\n",
        "print(f\"Simple CNN Test Accuracy: {test_acc}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZqIOIk6A2lgT",
        "outputId": "473896de-d568-4cd2-e959-ce14fcc68f03"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 1s/step - accuracy: 0.6013 - loss: 0.6900 - val_accuracy: 0.4500 - val_loss: 0.7302\n",
            "Epoch 2/10\n",
            "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 1s/step - accuracy: 0.7078 - loss: 0.5855 - val_accuracy: 0.6000 - val_loss: 0.5858\n",
            "Epoch 3/10\n",
            "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 1s/step - accuracy: 0.7846 - loss: 0.4839 - val_accuracy: 0.6000 - val_loss: 0.6923\n",
            "Epoch 4/10\n",
            "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 1s/step - accuracy: 0.8252 - loss: 0.4212 - val_accuracy: 0.7000 - val_loss: 0.6809\n",
            "Epoch 5/10\n",
            "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 1s/step - accuracy: 0.7882 - loss: 0.4852 - val_accuracy: 0.8000 - val_loss: 0.4389\n",
            "Epoch 6/10\n",
            "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 1s/step - accuracy: 0.8388 - loss: 0.3818 - val_accuracy: 0.7500 - val_loss: 0.4474\n",
            "Epoch 7/10\n",
            "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 1s/step - accuracy: 0.8509 - loss: 0.3722 - val_accuracy: 0.8000 - val_loss: 0.4956\n",
            "Epoch 8/10\n",
            "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 1s/step - accuracy: 0.8559 - loss: 0.3667 - val_accuracy: 0.8000 - val_loss: 0.3923\n",
            "Epoch 9/10\n",
            "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 1s/step - accuracy: 0.8432 - loss: 0.3767 - val_accuracy: 0.8000 - val_loss: 0.3864\n",
            "Epoch 10/10\n",
            "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 1s/step - accuracy: 0.8637 - loss: 0.3451 - val_accuracy: 0.8000 - val_loss: 0.3870\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 323ms/step - accuracy: 0.8000 - loss: 0.3870\n",
            "Simple CNN Test Accuracy: 0.800000011920929\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ResNet50 Model**"
      ],
      "metadata": {
        "id": "1WSjIJKf71z-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.applications import ResNet50\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
        "\n",
        "# Load the pre-trained ResNet50 model with ImageNet weights, exclude the top layers\n",
        "base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(128, 128, 3))\n",
        "\n",
        "# Add custom layers on top of ResNet50 base\n",
        "x = base_model.output\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "x = Dense(128, activation='relu')(x)\n",
        "x = Dropout(0.5)(x)\n",
        "predictions = Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "# Create the model\n",
        "resnet50_model = Model(inputs=base_model.input, outputs=predictions)\n",
        "\n",
        "# Freeze the layers of the base model\n",
        "for layer in base_model.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "# Compile the model\n",
        "resnet50_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "history_resnet = resnet50_model.fit(train_data, epochs=10, validation_data=test_data)\n",
        "\n",
        "# Save the model\n",
        "resnet50_model.save('resnet50_model.h5')\n",
        "\n",
        "# Evaluate the ResNet50 model\n",
        "test_loss_resnet, test_acc_resnet = resnet50_model.evaluate(test_data)\n",
        "print(f\"ResNet50 Test Accuracy: {test_acc_resnet}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YVBd7wkg4TFW",
        "outputId": "aecb2316-6528-4f79-dd3b-022781011a33"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "\u001b[1m94765736/94765736\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 0us/step\n",
            "Epoch 1/10\n",
            "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 2s/step - accuracy: 0.5315 - loss: 0.7500 - val_accuracy: 0.5000 - val_loss: 0.7222\n",
            "Epoch 2/10\n",
            "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 2s/step - accuracy: 0.5052 - loss: 0.7365 - val_accuracy: 0.5000 - val_loss: 0.6972\n",
            "Epoch 3/10\n",
            "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 2s/step - accuracy: 0.5570 - loss: 0.6912 - val_accuracy: 0.5000 - val_loss: 0.7348\n",
            "Epoch 4/10\n",
            "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 2s/step - accuracy: 0.5032 - loss: 0.7215 - val_accuracy: 0.5000 - val_loss: 0.6918\n",
            "Epoch 5/10\n",
            "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 2s/step - accuracy: 0.5638 - loss: 0.6879 - val_accuracy: 0.7000 - val_loss: 0.6891\n",
            "Epoch 6/10\n",
            "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 2s/step - accuracy: 0.4963 - loss: 0.6977 - val_accuracy: 0.5000 - val_loss: 0.6977\n",
            "Epoch 7/10\n",
            "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 2s/step - accuracy: 0.5605 - loss: 0.6895 - val_accuracy: 0.5000 - val_loss: 0.6937\n",
            "Epoch 8/10\n",
            "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 2s/step - accuracy: 0.4862 - loss: 0.6951 - val_accuracy: 0.5000 - val_loss: 0.6915\n",
            "Epoch 9/10\n",
            "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 2s/step - accuracy: 0.5498 - loss: 0.6915 - val_accuracy: 0.5000 - val_loss: 0.6927\n",
            "Epoch 10/10\n",
            "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 2s/step - accuracy: 0.5417 - loss: 0.6872 - val_accuracy: 0.5000 - val_loss: 0.6909\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - accuracy: 0.5000 - loss: 0.6909\n",
            "ResNet50 Test Accuracy: 0.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " **Task 2**"
      ],
      "metadata": {
        "id": "OLxWcEo_8SNm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import os\n",
        "import random\n",
        "import shutil\n",
        "import labelme2coco\n",
        "from mrcnn.config import Config\n",
        "from mrcnn import model as modellib, utils\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import imgaug  # for data augmentation\n",
        "import cv2  # for drawing masks\n",
        "\n"
      ],
      "metadata": {
        "id": "o-ZucjI5-vt7"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labelme_folder = '/content/drive/My Drive/Portfolio 5/log-labelled/log-labelled'"
      ],
      "metadata": {
        "id": "yDFcJYAm-1J0"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# List all files in the directory\n",
        "all_files = os.listdir(labelme_folder)"
      ],
      "metadata": {
        "id": "9H4TXDu7_SAl"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image_files = [f for f in all_files if f.endswith(('.jpg', '.png'))]\n",
        "annotation_files = [f for f in all_files if f.endswith('.json')]"
      ],
      "metadata": {
        "id": "TGdug8fL_Uhf"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "assert len(image_files) == len(annotation_files), \"Number of images and annotations don't match!\"\n"
      ],
      "metadata": {
        "id": "vmuNnXIE_aa-"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Randomly select 10 images for the test set\n",
        "test_images = random.sample(image_files, 10)\n",
        "\n",
        "# Create train and test directories\n",
        "train_image_dir = os.path.join('train', 'images')\n",
        "train_annotation_dir = os.path.join('train', 'annotations')\n",
        "test_image_dir = os.path.join('test', 'images')\n",
        "test_annotation_dir = os.path.join('test', 'annotations')\n",
        "\n",
        "os.makedirs(train_image_dir, exist_ok=True)\n",
        "os.makedirs(train_annotation_dir, exist_ok=True)\n",
        "os.makedirs(test_image_dir, exist_ok=True)\n",
        "os.makedirs(test_annotation_dir, exist_ok=True)"
      ],
      "metadata": {
        "id": "IGkt6Ugy_aVG"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to get annotation file corresponding to an image\n",
        "def get_annotation_file(image_file):\n",
        "    annotation_file = image_file.rsplit('.', 1)[0] + '.json'\n",
        "    if annotation_file in annotation_files:\n",
        "        return annotation_file\n",
        "    else:\n",
        "        raise FileNotFoundError(f\"Annotation file for image '{image_file}' not found.\")"
      ],
      "metadata": {
        "id": "-QpneGF8Cb0l"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Move test images and their corresponding annotations\n",
        "for img in test_images:\n",
        "    try:\n",
        "        annotation = get_annotation_file(img)\n",
        "        shutil.copy(os.path.join(labelme_folder, img), test_image_dir)\n",
        "        shutil.copy(os.path.join(labelme_folder, annotation), test_annotation_dir)\n",
        "    except FileNotFoundError as e:\n",
        "        print(e)\n",
        "\n",
        "# Move the remaining images and their corresponding annotations to the train set\n",
        "train_images = [img for img in image_files if img not in test_images]\n",
        "for img in train_images:\n",
        "    try:\n",
        "        annotation = get_annotation_file(img)\n",
        "        shutil.copy(os.path.join(labelme_folder, img), train_image_dir)\n",
        "        shutil.copy(os.path.join(labelme_folder, annotation), train_annotation_dir)\n",
        "    except FileNotFoundError as e:\n",
        "        print(e)"
      ],
      "metadata": {
        "id": "8sXLfYMaCh_E"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert Labelme annotations to COCO format\n",
        "coco_output_train = \"train/annotations_coco\"\n",
        "coco_output_test = \"test/annotations_coco\"\n",
        "\n",
        "labelme2coco.convert(train_annotation_dir, coco_output_train)\n",
        "labelme2coco.convert(test_annotation_dir, coco_output_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jdowKi12CmlR",
        "outputId": "032a6b97-c1f2-4a4e-ea0c-1b5a56a72767"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are 590 listed files in folder annotations.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Converting labelme annotations to COCO format: 100%|██████████| 590/590 [00:06<00:00, 87.71it/s] \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are 10 listed files in folder annotations.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Converting labelme annotations to COCO format: 100%|██████████| 10/10 [00:00<00:00, 46.80it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Mask RCNN Configuration\n",
        "class LogConfig(Config):\n",
        "    NAME = \"log\"\n",
        "    NUM_CLASSES = 1 + 1  # Background + log\n",
        "    GPU_COUNT = 1\n",
        "    IMAGES_PER_GPU = 1\n",
        "    STEPS_PER_EPOCH = 100\n",
        "    DETECTION_MIN_CONFIDENCE = 0.9\n",
        "    BACKBONE = \"resnet50\"\n",
        "    LEARNING_RATE = 0.001\n",
        "    IMAGE_MIN_DIM = 512\n",
        "    IMAGE_MAX_DIM = 512\n"
      ],
      "metadata": {
        "id": "W2wXkIolF8d7"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Dataset\n",
        "class LogDataset(utils.Dataset):\n",
        "    def load_logs(self, dataset_dir, subset):\n",
        "        self.add_class(\"log\", 1, \"log\")\n",
        "\n",
        "        # Add images\n",
        "        annotations_file = os.path.join(dataset_dir, \"annotations_coco.json\")\n",
        "        annotations = labelme2coco.load_json(annotations_file)\n",
        "        for annotation in annotations[\"images\"]:\n",
        "            image_path = os.path.join(dataset_dir, \"images\", annotation[\"file_name\"])\n",
        "            image_id = annotation[\"id\"]\n",
        "            self.add_image(\"log\", image_id=image_id, path=image_path, width=annotation[\"width\"], height=annotation[\"height\"])\n",
        "\n",
        "    def load_mask(self, image_id):\n",
        "        info = self.image_info[image_id]\n",
        "        if info[\"source\"] != \"log\":\n",
        "            return super(self.__class__, self).load_mask(image_id)\n",
        "\n",
        "        # Load mask annotations for the image\n",
        "        annotations = labelme2coco.load_json(os.path.join(os.path.dirname(info[\"path\"]), \"annotations_coco.json\"))\n",
        "        masks = []\n",
        "        for annotation in annotations[\"annotations\"]:\n",
        "            if annotation[\"image_id\"] == info[\"id\"]:\n",
        "                mask = np.zeros([info[\"height\"], info[\"width\"]], dtype=np.uint8)\n",
        "                for seg in annotation[\"segmentation\"]:\n",
        "                    polygon = np.array(seg).reshape((-1, 2))\n",
        "                    cv2.fillPoly(mask, [polygon], 1)\n",
        "                masks.append(mask)\n",
        "\n",
        "        if len(masks) == 0:\n",
        "            return super(self.__class__, self).load_mask(image_id)\n",
        "\n",
        "        masks = np.stack(masks, axis=-1)\n",
        "        class_ids = np.array([1] * masks.shape[-1], dtype=np.int32)\n",
        "        return masks, class_ids\n"
      ],
      "metadata": {
        "id": "NVLzLv9rF_9Q"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training\n",
        "config = LogConfig()\n",
        "\n",
        "# Fix the issue by using a functional approach to create the Mask RCNN model\n",
        "COCO_WEIGHTS_PATH = \"mask_rcnn_coco.h5\"\n",
        "model = modellib.MaskRCNN(mode=\"training\", config=config, model_dir=\"logs\")\n",
        "\n",
        "model.load_weights(COCO_WEIGHTS_PATH, by_name=True, exclude=[\"mrcnn_class_logits\", \"mrcnn_bbox_fc\", \"mrcnn_bbox\", \"mrcnn_mask\"])\n",
        "\n",
        "# Prepare dataset\n",
        "dataset_train = LogDataset()\n",
        "dataset_train.load_logs('train', \"train\")\n",
        "dataset_train.prepare()\n",
        "\n",
        "dataset_val = LogDataset()\n",
        "dataset_val.load_logs('test', \"test\")\n",
        "dataset_val.prepare()\n",
        "\n",
        "# Train the model\n",
        "with tf.device(\"/gpu:0\"):\n",
        "    model.train(dataset_train, dataset_val,\n",
        "                learning_rate=config.LEARNING_RATE,\n",
        "                epochs=30,\n",
        "                layers='heads',\n",
        "                augmentation=imgaug.augmenters.Fliplr(0.5))\n",
        "\n",
        "# Testing the model\n",
        "model_inference = modellib.MaskRCNN(mode=\"inference\", config=config, model_dir=\"logs\")\n",
        "model_inference.load_weights(COCO_WEIGHTS_PATH, by_name=True)\n",
        "\n",
        "# Test and visualize results on the test set\n",
        "for image_file in os.listdir(test_image_dir):\n",
        "    if image_file.endswith(('.jpg', '.png')):\n",
        "        image = cv2.imread(os.path.join(test_image_dir, image_file))\n",
        "        results = model_inference.detect([image], verbose=1)\n",
        "        r = results[0]\n",
        "\n",
        "        # Draw the detected objects on the image\n",
        "        visualize.display_instances(image, r['rois'], r['masks'], r['class_ids'],\n",
        "                                    dataset_val.class_names, r['scores'], show_bbox=True, show_mask=True)\n",
        "\n",
        "        # Save the output image with detections\n",
        "        output_path = os.path.join('output', image_file)\n",
        "        os.makedirs('output', exist_ok=True)\n",
        "        cv2.imwrite(output_path, image)\n",
        "\n",
        "        # Count the number of detected logs\n",
        "        num_logs = len(r['class_ids'])\n",
        "        print(f\"Number of detected logs in {image_file}: {num_logs}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 460
        },
        "id": "5UGEwBtAGDDA",
        "outputId": "b5377d5a-f7ab-415d-a373-2de8d3d002d6"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NotImplementedError",
          "evalue": "Exception encountered when calling Lambda.call().\n\n\u001b[1mWe could not automatically infer the shape of the Lambda's output. Please specify the `output_shape` argument for this Lambda layer.\u001b[0m\n\nArguments received by Lambda.call():\n  • args=('<KerasTensor shape=(None, None, 4), dtype=float32, sparse=None, name=input_gt_boxes>',)\n  • kwargs={'mask': 'None'}",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-73-7cecc6ea034d>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Fix the issue by using a functional approach to create the Mask RCNN model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mCOCO_WEIGHTS_PATH\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"mask_rcnn_coco.h5\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodellib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMaskRCNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"training\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"logs\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCOCO_WEIGHTS_PATH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mby_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexclude\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"mrcnn_class_logits\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"mrcnn_bbox_fc\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"mrcnn_bbox\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"mrcnn_mask\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/mrcnn/model.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, mode, config, model_dir)\u001b[0m\n\u001b[1;32m   1835\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_dir\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1836\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_log_dir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1837\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1838\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1839\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/mrcnn/model.py\u001b[0m in \u001b[0;36mbuild\u001b[0;34m(self, mode, config)\u001b[0m\n\u001b[1;32m   1873\u001b[0m                 shape=[None, 4], name=\"input_gt_boxes\", dtype=tf.float32)\n\u001b[1;32m   1874\u001b[0m             \u001b[0;31m# Normalize coordinates\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1875\u001b[0;31m             gt_boxes = KL.Lambda(\n\u001b[0m\u001b[1;32m   1876\u001b[0m     \u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnorm_boxes_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_image\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1877\u001b[0m     \u001b[0moutput_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Explicitly define the output shape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    120\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0;31m# `keras.config.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/lambda_layer.py\u001b[0m in \u001b[0;36mcompute_output_shape\u001b[0;34m(self, input_shape)\u001b[0m\n\u001b[1;32m     93\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m         \u001b[0;34m\"\"\"Returns the configuration of the layer for serialization.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNotImplementedError\u001b[0m: Exception encountered when calling Lambda.call().\n\n\u001b[1mWe could not automatically infer the shape of the Lambda's output. Please specify the `output_shape` argument for this Lambda layer.\u001b[0m\n\nArguments received by Lambda.call():\n  • args=('<KerasTensor shape=(None, None, 4), dtype=float32, sparse=None, name=input_gt_boxes>',)\n  • kwargs={'mask': 'None'}"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ymTONhkTZInf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}